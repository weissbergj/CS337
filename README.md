# BioDiligence: Predicting Drug Development Success

**CS337 AI for Healthcare | Team PhaseForward**

## What We're Building

This project tackles a real problem in drug development: most Phase III clinical trials fail, but there's no good way to predict which ones will succeed before investing millions of dollars and years of work. We're building a tool that uses historical data from ClinicalTrials.gov to predict whether a Phase II oncology trial is likely to successfully complete Phase III.

## The Big Question

Can we use early-stage trial data (drug names, cancer types, sponsor info, outcomes) to predict if that drug will make it through Phase III? Turns out, yesâ€”at least somewhat.

## The Data

We started with a massive dataset from ClinicalTrials.gov (via Kaggle)â€”over 496,000 clinical trials across all diseases. We filtered down to oncology trials only (anything tagged with "Neoplasm" in MeSH terms) and pulled out all Phase II and Phase III studies.

The tricky part was matching them up. We paired Phase II and Phase III trials that tested the same intervention (after cleaning up drug names to handle typos and formatting). If the Phase III trial's status was "COMPLETED", we labeled that as success. Otherwise, it's a failure.

We ended up with **5,071 matched pairs**â€”basically a 50/50 split between successes and failures, which is actually pretty close to real-world rates.

## What Goes Into the Model

We're working with six features from the Phase II trial:

**Text features** (we use TF-IDF to turn these into numbers):
- Drug/intervention names
- Trial title
- Cancer conditions being treated
- Primary outcome measures

**Categorical features**:
- Who's sponsoring it (industry, NIH, academic, etc.)
- Trial purpose (treatment, prevention, diagnostic, etc.)

We concatenate all the text fields together, run TF-IDF with a 5,000 feature limit, and one-hot encode the categorical stuff.

## The Model

Nothing fancy hereâ€”just logistic regression. We tried a few things, but honestly a simple model worked best for this dataset. We use:
- 80/20 train/test split (stratified so we keep the 50/50 balance)
- Balanced class weights (since we care about both successes and failures equally)
- 500 max iterations to make sure it converges

The model gets saved to `model.joblib` and you can retrain it by running `src/model/train.py`.

## How Well Does It Work?

On the test set:
- **Accuracy: 73%** (better than random!)
- **ROC-AUC: 0.79** (pretty decent for predicting something this uncertain)

It's not perfect, but it's way better than guessing. The model picks up on patterns like certain drug classes that tend to fail, or specific cancer types that are harder to treat successfully.

## What's in This Repo

Here's how everything's organized:

```
CS337/
â”œâ”€â”€ app.py                              # Main Streamlit app with calculator & dashboard
â”œâ”€â”€ model.joblib                        # Our trained model
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ phase2_phase3_pairs.csv        # 5,071 matched Phase IIâ†’III pairs
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ dashboard.py               # Historical insights dashboard
    â”‚   â”œâ”€â”€ data_loader.py             # Data loading utilities
    â”‚   â””â”€â”€ mock_data.py               # Sample data for demo
    â”‚
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ build_pairs.py             # Script to create Phase IIâ†’III matches
    â”‚   â””â”€â”€ explore_features.py        # Data exploration notebooks
    â”‚
    â”œâ”€â”€ model/
    â”‚   â”œâ”€â”€ train.py                   # Model training pipeline
    â”‚   â””â”€â”€ feature_importance.py      # Extract important features
    â”‚
    â””â”€â”€ visuals/
        â””â”€â”€ plot_feature_importance.py # Generate visualizations
```

## Try It Out

We built a Streamlit app with two main features:

### 1. Success Predictor
Enter details about a Phase II trial (drug name, cancer type, sponsor, outcomes) and get an instant probability estimate for Phase III success. The interface is simpleâ€”just fill in the fields and hit predict.

### 2. Historical Insights Dashboard
This is where things get interesting. We built an interactive dashboard with 10 different analysis views to explore what actually drives success rates in oncology trials:

- **ğŸ“Š Overview** - High-level stats on trial counts, success rates, and basic distribution patterns
- **ğŸ¢ Sponsor Analysis** - Compare success rates across different sponsor types (industry vs academic vs government)
- **ğŸ’Š Top Interventions** - Which drugs and drug combinations have the highest success rates? See the top performers and biggest failures
- **ğŸ“ˆ Advanced Analytics** - Dig into model performance with ROC curves, feature importance, and prediction distributions
- **ğŸ¯ Cancer Type Deep Dive** - Success rates vary wildly by cancer type. See which cancers are easier vs harder to treat successfully
- **ğŸ”¬ Intervention Patterns** - Analyze patterns in drug naming, combination therapies, and treatment approaches
- **ğŸ“… Temporal Trends** - How have success rates changed over time? Are we getting better at predicting winners?
- **ğŸ“‹ Trial Status Analysis** - Break down trials by their current status (completed, terminated, withdrawn, etc.)
- **ğŸŒ Organization Insights** - Geographic patterns and institutional success rates
- **ğŸ”— Correlation Matrix** - Explore relationships between different features and success rates

Each tab has interactive visualizations built with Plotly, so you can hover, zoom, and explore the data yourself.

### Running It

The app is deployed on Streamlit Cloud (link in repo) or you can run it locally:

```bash
pip install -r requirements.txt
streamlit run app.py
```

## Team

**PhaseForward:** Charles Chen, Chelsea Hu, Meghana Paturu, and Jared Weissberg

Built for Stanford CS337 â€“ AI for Healthcare, Fall 2024
